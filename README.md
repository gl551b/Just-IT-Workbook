## Just IT Workbook



# Excel

<div style="display:flex; gap:10px;">
    <img src="images/excel%20(1).png" height="180">
    <img src="images/excel%20(2).png" height="180">
    <img src="images/excel%20(3).png" height="180">
</div>
<br />

In this coursework, I developed a comprehensive Excel framework for data analysis and performance reporting. The project involved end-to-end data management, starting with cleaning and validating raw datasets to ensure accuracy. I then built dynamic, formatted tables with integrated filters and slicers to facilitate intuitive, interactive data exploration.

A core component was leveraging Pivot Tables to perform sophisticated sales analysis, dissecting performance by key demographics such as gender, age group, and country. To automate insights, I utilized a range of functions from basic (SUM, AVERAGE) to advanced (SWITCH, conditional logic) for dynamic calculations and categorizations.

The findings were synthesized into a Sales Summary dashboard, integrating key charts and KPIs for at-a-glance performance reporting. Throughout the process, I prioritized data security, adhering to GDPR and DPA 2018 guidelines. Finally, I focused on effectively communicating the results through structured summaries and explored tools like Power BI to further enhance the impact of data visualizations for stakeholders.


# Tableau

<img src="images/tableau.png" width="500">
<br />

In my Tableau coursework, I undertook a comprehensive project that began by connecting and preparing multiple diverse datasets, including labor market data from EMSI JobChange UK and user engagement metrics from Spotify. This foundational step involved meticulous data modeling to establish the correct relationships between tables, ensuring a robust and accurate foundation for analysis. I then progressed to building a series of interactive dashboards, strategically combining bar charts, geographical maps, and dynamic filters to create a cohesive analytical environment.

To transform this raw data into clear, actionable intelligence, I leveraged Tableau's advanced functionalities. I created calculated fields to derive key metrics such as percentage change, and used strategic color encoding to visually highlight trends and outliers, making complex patterns immediately understandable. A significant part of the process involved exploring different levels of data granularity and aggregation, allowing me to refine the visuals from high-level overviews down to detailed, specific insights. This ensured the final output was not just visually compelling, but also analytically sound.

The ultimate objective was to design presentation-ready visuals tailored for non-technical stakeholders. I focused on storytelling through multiple, linked visualizations, enabling users to interact with the data through intuitive filters and slicers. This approach empowered them to explore the information themselves, fostering a data-driven dialogue. By publishing and sharing these dashboards, I successfully demonstrated my ability to translate exploratory data analysis into strategic, accessible insights, showcasing the full power of Tableau for visual analytics and effective dashboard design.


# Power BI

<div style="display:flex; gap:10px;">
    <img src="images/power%20bi%20(2).jpg" height="300">
    <img src="images/power%20bi%20(1).jpg" height="300">
</div>
<br />

My journey into this Power BI coursework began not with charts, but with data itself. I started by connecting to diverse sources, including Excel and Azure SQL, to establish a real-time analytics foundation. Within Power Query, I meticulously cleaned and transformed this raw data, performing null checks and validation to ensure integrity before loading it into the model. The true structural work began when I engineered a robust data model by carefully managing relationships between key tables like Reseller, Sales, and Product, creating a solid backbone for all subsequent analysis. To unlock deeper insights, I progressed to creating calculated columns and DAX measures, building the essential formulas to track profitability and other critical KPIs.

With a reliable data model in place, I transitioned to the art of visual storytelling. In Power BI Desktop, I designed a comprehensive, multi-chart dashboard to illuminate key business narratives, such as monthly sales trends and category performance. This was more than a static report; it was an interactive experience. I empowered the visuals with filters, slicers, and conditional formatting, allowing each chart to communicate with the others. This approach transformed the dashboard into a dynamic tool for exploration, where stakeholders could drill down from high-level trends to granular details simply by clicking on the data that intrigued them.

The final stage was about synthesis and delivery, moving from a technical build to a platform for insight. I published and shared the finished dashboard, understanding that its true value lies in its ability to communicate. This project was a complete immersion into the Power BI ecosystem, from the foundational principles of data modeling and transformation to the advanced application of DAX and interactive visualization. The key takeaway was the ability to build insight-driven visual reports that are not only accurate but also deliver clear business value, empowering stakeholders to explore data and make informed, data-driven decisions with confidence.


# MySQL

<div style="display:flex; gap:10px;">
    <img src="images/sql%20model%20(2).png" height="200">
    <img src="images/sql%20(3).png" height="200">
    <img src="images/sql%20(4).png" height="200">
</div>
<br />

My coursework provided a comprehensive foundation in database management, beginning with the core principles that distinguish relational from non-relational systems. I learned the essential terminology such as tables, schemas, and the critical roles of primary, foreign, and secondary keys which equipped me to understand and design data relationships. This theoretical knowledge was immediately put into practice as I defined a database schema from the ground up using `CREATE DATABASE` and `CREATE TABLE` commands, consciously applying these keys to enforce relational integrity and model real-world data connections.

Moving into practical application, I became proficient in SQL, mastering everything from fundamental data retrieval to advanced analytical queries. I extensively used `SELECT` statements with `WHERE`, `LIKE`, and `BETWEEN` to filter data, and implemented aggregate functions like `COUNT`, `SUM`, and `AVG` to derive summary metrics. A significant part of my learning involved joining multiple tables; I practiced all major `JOIN` types including INNER, LEFT, RIGHT, FULL, SELF, and CROSS to combine data and create comprehensive reports. Furthermore, I honed my skills in handling `NULL` values and constructing subqueries, which were essential for performing complex, multi-layered data analysis.

The culmination of this work was a hands-on project where I designed and populated a retail database, linking tables for Inventory, Sales, and Customers through Product and Customer IDs. Using `INSERT INTO` statements, I populated the tables and then crafted a series of scenario-based queries to answer specific business and geographic questions, such as analyzing demographic patterns and sales performance. This end-to-end process from schema design and data population to writing sophisticated queries for aggregated reports significantly enhanced my problem-solving and debugging abilities, solidifying my capacity to translate business requirements into accurate, effective SQL code.


# Azure

| Topic | Core Skills | Key Takeaways |
| :--- | :--- | :--- |
| **Cloud Services** | Gained proficiency in core Azure services, including: **Storage** (Blob Storage, Data Lake), **Compute** (Virtual Machines, App Services) and **Managed Data** (Azure SQL Database, Synapse Analytics) | Understood how to integrate various cloud services into a cohesive data workflow to enable powerful and scalable analytics solutions. |
| **Data Deployment** | Developed skills in deploying data pipelines and strategically storing datasets in the cloud. | Learned to make critical decisions on where and how to host and process data, incorporating basic security and authentication principles like service principals and access keys. |
<br />

The Azure coursework began with a foundational exploration of cloud computing, where I moved from understanding its core characteristics to critically evaluating the strategic distinctions between Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS) models. This foundational knowledge was immediately applied to a comparative analysis of major providers like AWS and Azure, framed within the critical context of modern data legislation such as GDPR and the DPA 2018. I learned that a robust cloud solution is not just about technological capability but also about ensuring compliance, security, and the ethical stewardship of user data from the ground up.

Building on this, I delved into the architectural specifics of Azure, learning to map its extensive service catalog to real-world data challenges. I gained hands-on experience designing a complete cloud data architecture for a case study, "Paws & Whiskers," which involved selecting optimal Azure services like Blob Storage for raw data, Azure SQL Database for structured information, and Synapse Analytics for large-scale processing. This required me to differentiate between relational and non-relational data models and to plan a detailed migration strategy from legacy systems like Excel to the cloud, selecting efficient storage formats like Parquet and ensuring security through encryption and service principals for authentication.

The coursework culminated in synthesizing these components into a resilient and scalable data pipeline. Using services like Azure Data Factory, I outlined the deployment of data workflows that could ingest, transform, and prepare data for analytics in Power BI. Furthermore, I addressed the operational longevity of these systems by developing comprehensive strategies for disaster recovery using Azure Backup and Site Recovery, and by planning for future scalability. This end-to-end process transformed my theoretical knowledge into a practical, narrative understanding of how to build, secure, and maintain enterprise-grade data solutions in the Microsoft Azure ecosystem.

# Python

<img src="images/python%20(4).png" width="680">

<div style="display:flex; gap:10px;">
    <img src="images/python%20(3).png" height="260">
    <img src="images/python%20(5).png" height="260">
    <img src="images/python%20(1).png" height="260">
</div>
<br />

My journey into data analysis with Python was a deep, practical immersion into transforming raw data into actionable intelligence. I became proficient in using the pandas library as my primary tool, where I learned to programmatically manipulate and analyze datasets through core DataFrame operations. This involved the entire data lifecycle: from importing data with read_csv and performing initial assessments using head and describe, to the critical stage of data cleaning. I practiced intricate data interrogation by indexing, slicing, and using conditional selection, and progressed to reshaping data by adding, renaming, and dropping columns to create new, derived fields that were more meaningful for my analysis.

To extract meaningful patterns and insights, I moved beyond basic manipulation to more advanced analytical techniques. I harnessed the power of aggregation and grouping, for instance, to calculate average marks by class or gender, and built pivot tables to summarize complex relationships within the data. A significant part of this process was strengthening my programming logic; I developed custom functions and implemented control flow using conditional if statements and iteration loops to automate decision-making, such as creating a robust grade classification system. This scripting proficiency allowed me to automate repetitive tasks, ensuring data was consistently prepared and ready for reporting, which I finalized by sorting and exporting refined datasets to CSV.

The final phase of my work was dedicated to communication and collaboration. Using Matplotlib and Seaborn, I translated my numerical findings into clear, basic visualizations, making the trends and conclusions evident at a glance. Throughout this entire project, my workflow was centered in Jupyter Notebook, which honed my skills in syntax, debugging, and writing readable, well-documented code. Furthermore, this technical foundation was reinforced through collaborative group labs, where I applied these data-handling skills to solve problems collectively, solidifying not only my technical expertise but also the crucial ability to leverage Python's libraries for effective, data-driven storytelling.

